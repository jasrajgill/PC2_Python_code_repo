{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec00a0d1",
   "metadata": {},
   "source": [
    "# CNZS Pilot Test Survey ‚Äì Long Format Transformation\n",
    "This notebook transposes survey responses into long format with question metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b6ca6",
   "metadata": {},
   "source": [
    "## Step 0: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae7a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a096841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL CONFIG (to be used later in Step - 5&7)\n",
    "# ============================================================\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"Mapping Description\", \"Mapping Group\", \"Taskforce Person\",\n",
    "    \"Content Reference\", \"Q Type\", \"Group Quanti?\", \"Question Options\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18adf13e",
   "metadata": {},
   "source": [
    "## Step 1: Load Data & Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6533f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START STEP 1 ==========\n",
      "üìÑ Loaded cleaned dataset: (26, 431)\n",
      "üü¶ Boolean-like columns: 279\n",
      "üü© Converted boolean values to TRUE/FALSE\n",
      "üìò Loaded question mapping: (441, 13)\n",
      "‚úÖ All metadata columns found\n",
      "üíæ Saved cleaned mapping\n",
      "========== STEP 1 COMPLETE ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1 ‚Äî MASTER LOADER PIPELINE (Modular & Clean)\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Clean and standardize key metadata question wording\n",
    "# ----------------------------------------------------\n",
    "def clean_question_text(text):\n",
    "    \"\"\"Standardize key metadata questions so naming stays consistent.\"\"\"\n",
    "    replacements = {\n",
    "        \"What country is your organization headquartered in? If you are responding in a personal capacity, select the country where you are based.\": \"Country\",\n",
    "\n",
    "        \"If you are a Corporate, Financial Institution, or Professional Service & Consultancy:What sector does your company operate in?\": \"Sector\",\n",
    "\n",
    "        \"If you are a Corporate, Financial Institution, or Professional Service & Consultancy:What is your company‚Äôs SBTi status?\": \"SBTi Status\",\n",
    "\n",
    "        \"If you are a Corporate, Financial Institution, or Professional Service & Consultancy:Select the range that best represents your total number of full-time employees in your most recent reporting year.\": \"Employees MRY\",\n",
    "\n",
    "        \"If you are a Corporate, Financial Institution, or Professional Service & Consultancy:What was your company‚Äôs net annual turnover in the most recent reporting year(revenue)?\": \"Annual Turnover MRY\",\n",
    "    }\n",
    "    return replacements.get(text, text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Identify boolean-like columns (values only 0/1)\n",
    "# ----------------------------------------------------\n",
    "def identify_boolean_columns(df):\n",
    "    bool_cols = [\n",
    "        col for col in df.columns \n",
    "        if set(df[col].dropna().unique()).issubset({0, 1})\n",
    "    ]\n",
    "    print(f\"üü¶ Boolean-like columns: {len(bool_cols)}\")\n",
    "    return bool_cols\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Convert 0/1 ‚Üí TRUE/FALSE\n",
    "# ----------------------------------------------------\n",
    "def convert_bool_columns(df, bool_cols):\n",
    "    df[bool_cols] = df[bool_cols].applymap(\n",
    "        lambda x: \"TRUE\" if x == 1 else (\"FALSE\" if x == 0 else x)\n",
    "    )\n",
    "    print(\"üü© Converted boolean values to TRUE/FALSE\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Load cleaned dataset\n",
    "# ----------------------------------------------------\n",
    "def load_cleaned_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"‚ùå Missing cleaned dataset: {path}\")\n",
    "    df = pd.read_excel(path)\n",
    "    print(f\"üìÑ Loaded cleaned dataset: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Load question mapping + clean metadata question text\n",
    "# ----------------------------------------------------\n",
    "def load_question_mapping(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"‚ùå Mapping doc not found at: {path}\")\n",
    "\n",
    "    xl = pd.ExcelFile(path)\n",
    "    df_map = pd.read_excel(xl, sheet_name=\"mapping_output\")\n",
    "\n",
    "    df_map.columns = [\n",
    "        \"Question ID\", \"Question\", \"OG Question Number\", \"Section\", \"Subsection\", \"Mapping Description\",\n",
    "        \"Mapping Group\", \"Question Group\", \"Taskforce Person\", \"Content Reference\",\n",
    "        \"Q Type\", \"Group Quanti?\", \"Question Options\"\n",
    "    ]\n",
    "\n",
    "    # Apply your metadata-cleaning helper\n",
    "    df_map[\"Question\"] = df_map[\"Question\"].apply(clean_question_text)\n",
    "\n",
    "    print(f\"üìò Loaded question mapping: {df_map.shape}\")\n",
    "    return df_map\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Validate metadata columns exist in dataset\n",
    "# ----------------------------------------------------\n",
    "def validate_metadata(df_cleaned):\n",
    "    meta_cols = [\n",
    "        \"Response ID\", \"Country\", \"Sector\", \"SBTi Status\",\n",
    "        \"Employees MRY\", \"Annual Turnover MRY\",\n",
    "        \"Category A/B\", \"World Bank Income Group\", \"Region\"\n",
    "    ]\n",
    "\n",
    "    missing = set(meta_cols) - set(df_cleaned.columns)\n",
    "    if missing:\n",
    "        print(f\"‚ö† Missing metadata columns: {missing}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All metadata columns found\")\n",
    "\n",
    "    return meta_cols\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Save intermediate mapping file\n",
    "# ----------------------------------------------------\n",
    "def save_intermediate(question_map):\n",
    "    os.makedirs(\"intermediate_checks\", exist_ok=True)\n",
    "    question_map.to_excel(\n",
    "        \"intermediate_checks/question_mapping_cleaned.xlsx\", index=False\n",
    "    )\n",
    "    print(\"üíæ Saved cleaned mapping\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MASTER FUNCTION ‚Äî runs full Step 1\n",
    "# ----------------------------------------------------\n",
    "def Step1_load_and_prepare(\n",
    "    cleaned_path=\"PC2_cleaned_full_dataset_anonymized.xlsx\",\n",
    "    mapping_path=\"../input/PC2_analysis_mapping_doc.xlsx\"\n",
    "):\n",
    "    print(\"========== START STEP 1 ==========\")\n",
    "\n",
    "    df = load_cleaned_dataset(cleaned_path)\n",
    "\n",
    "    bool_cols = identify_boolean_columns(df)\n",
    "    df = convert_bool_columns(df, bool_cols)\n",
    "\n",
    "    qmap = load_question_mapping(mapping_path)\n",
    "\n",
    "    meta_cols = validate_metadata(df)\n",
    "\n",
    "    save_intermediate(qmap)\n",
    "\n",
    "    print(\"========== STEP 1 COMPLETE ==========\\n\")\n",
    "    return df, qmap, meta_cols\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Execute Step 1\n",
    "# ----------------------------------------------------\n",
    "df_cleaned, question_map, meta_cols = Step1_load_and_prepare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11ed69",
   "metadata": {},
   "source": [
    "## Step 2: Assign question numbers to data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efaff208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START STEP 2: Assign Question IDs ==========\n",
      "üü¶ Assigned Question IDs to 431 columns\n",
      "üîó Merged mapping: (437, 14)\n",
      "üü© Valid questions: 196\n",
      "üü® Quanti questions: 77\n",
      "‚úÖ All valid questions exist in df_cleaned\n",
      "========== STEP 2 COMPLETE ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 ‚Äî Assign Question Numbers + Validate Mappings\n",
    "# ============================================================\n",
    "\n",
    "def Step2_assign_question_numbers(df_cleaned, question_map, meta_cols, removed_questions=None):\n",
    "    print(\"========== START STEP 2: Assign Question IDs ==========\")\n",
    "\n",
    "    if removed_questions is None:\n",
    "        removed_questions = []\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Create numeric ID for each column in df_cleaned\n",
    "    # --------------------------------------------------------\n",
    "    column_numbers = {\n",
    "        col: idx + 1 for idx, col in enumerate(df_cleaned.columns)\n",
    "    }\n",
    "\n",
    "    column_map_df = (\n",
    "        pd.DataFrame(column_numbers.items(), columns=[\"Question\", \"Question ID\"])\n",
    "    )\n",
    "\n",
    "    print(f\"üü¶ Assigned Question IDs to {len(column_map_df)} columns\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Merge with question_map\n",
    "    # --------------------------------------------------------\n",
    "    merged_map = pd.merge(\n",
    "        column_map_df,\n",
    "        question_map,\n",
    "        on=\"Question\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(f\"üîó Merged mapping: {merged_map.shape}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Create masks for valid + quanti questions\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Normalize groups\n",
    "    qmap_group = question_map[\"Mapping Group\"].astype(str).str.lower()\n",
    "    qmap_qtype = question_map[\"Q Type\"].astype(str).str.lower()\n",
    "\n",
    "    # Mask for valid (non-metadata, non-excluded)\n",
    "    mask_valid_questions = ~qmap_group.isin([\"meta\", \"not included\", \"survey feedback\"])\n",
    "\n",
    "    # Mask for quanti questions\n",
    "    mask_quanti_questions = (\n",
    "        qmap_qtype.eq(\"quanti\")\n",
    "        & ~question_map[\"Question\"].isin(removed_questions)\n",
    "        & ~question_map[\"Question\"].isin(meta_cols)\n",
    "        & ~qmap_group.isin([\"meta\", \"not included\", \"survey feedback\"])\n",
    "    )\n",
    "\n",
    "    valid_question_columns = (\n",
    "        question_map[mask_valid_questions][\"Question\"].dropna().unique().tolist()\n",
    "    )\n",
    "\n",
    "    quanti_question_columns = (\n",
    "        question_map[mask_quanti_questions][\"Question\"].dropna().unique().tolist()\n",
    "    )\n",
    "\n",
    "    print(f\"üü© Valid questions: {len(valid_question_columns)}\")\n",
    "    print(f\"üü® Quanti questions: {len(quanti_question_columns)}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Validate that all mapping questions exist in df_cleaned\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    q_errors = [\n",
    "        q for q in valid_question_columns\n",
    "        if q not in df_cleaned.columns\n",
    "    ]\n",
    "\n",
    "    if q_errors:\n",
    "        print(f\"‚ö†Ô∏è WARNING: {len(q_errors)} mapped questions NOT in dataset\")\n",
    "        os.makedirs(\"intermediate_checks\", exist_ok=True)\n",
    "        pd.DataFrame(q_errors, columns=[\"Missing Questions\"]).to_csv(\n",
    "            \"intermediate_checks/q_errors.csv\", index=False\n",
    "        )\n",
    "        print(\"   ‚Üí Saved missing question list to intermediate_checks/q_errors.csv\")\n",
    "    else:\n",
    "        print(\"‚úÖ All valid questions exist in df_cleaned\")\n",
    "\n",
    "    print(\"========== STEP 2 COMPLETE ==========\\n\")\n",
    "\n",
    "    return (\n",
    "        column_map_df,\n",
    "        merged_map,\n",
    "        valid_question_columns,\n",
    "        quanti_question_columns,\n",
    "        q_errors\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Execute Step 2\n",
    "# ------------------------------------------------------------\n",
    "column_map_df, merged_map, valid_questions, quanti_questions, q_errors = Step2_assign_question_numbers(\n",
    "    df_cleaned=df_cleaned,\n",
    "    question_map=question_map,\n",
    "    meta_cols=meta_cols,\n",
    "    removed_questions=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8d0b9",
   "metadata": {},
   "source": [
    "## Step 3: Transpose survey to long format with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb57cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START STEP 3: Long Format Transformation ==========\n",
      "üìÑ Long format dataset created: (5096, 11)\n",
      "üßπ Cleaned merged long dataset: (2050, 23)\n",
      "üíæ Saved long-format dataset to intermediate_checks/long_format_transposed.xlsx\n",
      "üíæ Saved progress log to intermediate_checks/question_group_analysis_progress_log.xlsx\n",
      "========== STEP 3 COMPLETE ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 3 ‚Äî Transpose to Long Format + Progress Log\n",
    "# ============================================================\n",
    "\n",
    "def Step3_transpose_long_format(\n",
    "    df_cleaned,\n",
    "    question_map,\n",
    "    meta_cols,\n",
    "    valid_question_columns,\n",
    "    mask_valid_questions,\n",
    "    output_dir=\"intermediate_checks\"\n",
    "):\n",
    "    print(\"========== START STEP 3: Long Format Transformation ==========\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Create long-format dataset\n",
    "    # ------------------------------------------------------------\n",
    "    df_long = df_cleaned[meta_cols + valid_question_columns].melt(\n",
    "        id_vars=meta_cols,\n",
    "        var_name=\"Question\",\n",
    "        value_name=\"Answer\"\n",
    "    ).copy()\n",
    "\n",
    "    print(f\"üìÑ Long format dataset created: {df_long.shape}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Merge long format with question_map\n",
    "    # ------------------------------------------------------------\n",
    "    valid_mapping = question_map[\n",
    "        question_map[\"Question\"].isin(valid_question_columns)\n",
    "    ]\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_long,\n",
    "        valid_mapping,\n",
    "        on=\"Question\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Remove \"index\" pseudo-row if it appears\n",
    "    df_merged = df_merged[df_merged[\"Question\"] != \"index\"]\n",
    "\n",
    "    # Remove rows with missing answers\n",
    "    df_merged = df_merged[df_merged[\"Answer\"].notna()]\n",
    "\n",
    "    print(f\"üßπ Cleaned merged long dataset: {df_merged.shape}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Check for valid questions that never appear in df_merged\n",
    "    # ------------------------------------------------------------\n",
    "    missing_in_long = [\n",
    "        q for q in valid_question_columns\n",
    "        if q not in df_merged[\"Question\"].unique()\n",
    "    ]\n",
    "\n",
    "    if missing_in_long:\n",
    "        print(f\"‚ö†Ô∏è {len(missing_in_long)} valid questions never appear in long format.\")\n",
    "        pd.DataFrame(missing_in_long, columns=[\"Missing Questions\"]).to_csv(\n",
    "            f\"{output_dir}/missing_in_long_format.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # Save merged long-format for inspection\n",
    "    df_merged.to_excel(f\"{output_dir}/long_format_transposed.xlsx\", index=False)\n",
    "    print(f\"üíæ Saved long-format dataset to {output_dir}/long_format_transposed.xlsx\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. Build Question Group Progress Log\n",
    "    # ------------------------------------------------------------\n",
    "    question_map_log = question_map.copy()\n",
    "\n",
    "    # Keep only valid (non-meta, non-excluded) mapping groups\n",
    "    question_map_log = question_map_log[mask_valid_questions].copy()\n",
    "\n",
    "    # One row per mapping group (not per question)\n",
    "    question_map_log = question_map_log.drop_duplicates(\n",
    "        subset=[\"Mapping Group\"]\n",
    "    )\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    keep_cols = [\n",
    "        \"Mapping Group\",\n",
    "        \"Taskforce Person\",\n",
    "        \"Content Reference\"\n",
    "    ]\n",
    "\n",
    "    question_map_log = question_map_log[keep_cols].copy()\n",
    "\n",
    "    # Rename Content Reference ‚Üí Topic Owner\n",
    "    question_map_log.rename(\n",
    "        columns={\"Content Reference\": \"Topic Owner\"},\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Add empty workflow / progress columns\n",
    "    # ------------------------------------------------------------\n",
    "    progress_cols = [\n",
    "        \"Group contains mixed quanti/quali? (y/n)\",\n",
    "        \"Initial topic identification: complete? (y/n)\",\n",
    "        \"Review by topic owner(s): complete? (y/n)\",\n",
    "        \"Final topic list: complete? (y/n)\",\n",
    "        \"Qualitative Stage 2: complete? (y/n)\",\n",
    "        \"Quantitative analysis: complete? (y/n)\"\n",
    "    ]\n",
    "\n",
    "    for col in progress_cols:\n",
    "        question_map_log[col] = None\n",
    "\n",
    "    # Save progress log\n",
    "    out_path = f\"{output_dir}/question_group_analysis_progress_log.xlsx\"\n",
    "    question_map_log.to_excel(out_path, index=False)\n",
    "\n",
    "    print(f\"üíæ Saved progress log to {out_path}\")\n",
    "    print(\"========== STEP 3 COMPLETE ==========\\n\")\n",
    "\n",
    "    return df_merged, question_map_log, missing_in_long\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Execute Step 3\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "merged_filtered, question_group_log, missing_in_long = Step3_transpose_long_format(\n",
    "    df_cleaned=df_cleaned,\n",
    "    question_map=question_map,\n",
    "    meta_cols=meta_cols,\n",
    "    valid_question_columns=valid_questions,\n",
    "    mask_valid_questions=question_map[\"Mapping Group\"]\n",
    "        .astype(str).str.lower()\n",
    "        .isin([\"meta\", \"not included\", \"survey feedback\"])\n",
    "        .__invert__()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61782da2",
   "metadata": {},
   "source": [
    "# Step 4: Count valid responses per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29aefec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4 ‚Äî Valid Response Counts + Crosstabs + Graphs\n",
    "# ============================================================\n",
    "\n",
    "def Step4_generate_summary_tables(merged_filtered, df_cleaned):\n",
    "    print(\"========== START STEP 4: Response Summary ==========\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Base Summary Table\n",
    "    # ------------------------------------------------------------\n",
    "    summary_table = (\n",
    "        merged_filtered.groupby([\"Question ID\", \"Question\", \"Q Type\"])\n",
    "        .agg(valid_responses=(\"Answer\", \"count\"))\n",
    "        .reset_index()\n",
    "        .sort_values(\"Question ID\")\n",
    "    )\n",
    "\n",
    "    summary_table[\"total_responses\"] = len(df_cleaned)\n",
    "    summary_table[\"response_rate\"] = (\n",
    "        summary_table[\"valid_responses\"] / summary_table[\"total_responses\"] * 100\n",
    "    )\n",
    "\n",
    "    summary_table.to_csv(\"valid_responses_count.csv\", index=False)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Quali + Quanti Split\n",
    "    # ------------------------------------------------------------\n",
    "    df_quali = summary_table[summary_table[\"Q Type\"] == \"quali\"].copy()\n",
    "    df_quali[\">200\"] = df_quali[\"valid_responses\"] > 200\n",
    "    df_quali.to_csv(\"quali_valid_responses_count.csv\", index=False)\n",
    "\n",
    "    df_quanti = summary_table[summary_table[\"Q Type\"] == \"quanti\"].copy()\n",
    "    df_quanti.to_csv(\"quanti_valid_responses_count.csv\", index=False)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Histogram\n",
    "    # ------------------------------------------------------------\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.hist(\n",
    "        [\n",
    "            summary_table[summary_table[\"Q Type\"] == \"quali\"][\"valid_responses\"],\n",
    "            summary_table[summary_table[\"Q Type\"] == \"quanti\"][\"valid_responses\"],\n",
    "        ],\n",
    "        bins=50,\n",
    "        label=[\"Qualitative\", \"Quantitative\"],\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.xlabel(\"Valid Response Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Valid Response Counts by Question Type\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. World Bank Income Group Breakdown\n",
    "    # ------------------------------------------------------------\n",
    "    wb_order = [\"High income\", \"Upper middle income\", \"Lower middle income\"]\n",
    "\n",
    "    summary_wb = (\n",
    "        merged_filtered.groupby(\n",
    "            [\"Question ID\", \"Question\", \"Q Type\", \"World Bank Income Group\"]\n",
    "        )\n",
    "        .agg(valid_responses=(\"Answer\", \"count\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    wb_pivot = (\n",
    "        summary_wb.pivot(\n",
    "            index=\"Question ID\",\n",
    "            columns=\"World Bank Income Group\",\n",
    "            values=\"valid_responses\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .reindex(columns=wb_order)\n",
    "    )\n",
    "\n",
    "    wb_pivot[\"Total\"] = wb_pivot.sum(axis=1)\n",
    "    wb_pivot.T.to_excel(\"PC2_question_answers_by_wb_income_group.xlsx\")\n",
    "    wb_pivot.T.to_csv(\"PC2_question_answers_by_wb_income_group.csv\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Region Breakdown\n",
    "    # ------------------------------------------------------------\n",
    "    region_order = [\n",
    "        \"Europe\",\n",
    "        \"Northern America\",\n",
    "        \"Asia\",\n",
    "        \"Latin America and the Caribbean\",\n",
    "        \"Africa\",\n",
    "        \"Oceania\",\n",
    "        \"MENA\",\n",
    "    ]\n",
    "\n",
    "    summary_reg = (\n",
    "        merged_filtered.groupby(\n",
    "            [\"Question ID\", \"Question\", \"Q Type\", \"Region\"]\n",
    "        )\n",
    "        .agg(valid_responses=(\"Answer\", \"count\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    reg_pivot = (\n",
    "        summary_reg.pivot(index=\"Question ID\", columns=\"Region\", values=\"valid_responses\")\n",
    "        .fillna(0)\n",
    "        .reindex(columns=region_order)\n",
    "    )\n",
    "\n",
    "    reg_pivot[\"Total\"] = reg_pivot.sum(axis=1)\n",
    "    reg_pivot.T.to_excel(\"PC2_question_answers_by_region.xlsx\")\n",
    "    reg_pivot.T.to_csv(\"PC2_question_answers_by_region.csv\")\n",
    "\n",
    "    print(\"========== STEP 4 COMPLETE ==========\\n\")\n",
    "\n",
    "    return summary_table, df_quali, df_quanti, wb_pivot, reg_pivot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b538a",
   "metadata": {},
   "source": [
    "# Step 5: Export transposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba79dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START STEP 5: Export Transposed Dataset ==========\n",
      "üíæ Saved: PC2_all_responses_meta_data_one_answer_per_line.xlsx / .csv\n",
      "========== STEP 5 COMPLETE ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 5 ‚Äî Export Clean Transposed Dataset (One Answer Per Line)\n",
    "# ============================================================\n",
    "\n",
    "def Step5_export_transposed_dataset(merged_filtered):\n",
    "    print(\"========== START STEP 5: Export Transposed Dataset ==========\")\n",
    "\n",
    "    # Work on a copy\n",
    "    df_save = merged_filtered.copy()\n",
    "\n",
    "    # Replace NaN with \"N/A\" for better readability outside Python\n",
    "    df_save.replace(np.nan, \"N/A\", inplace=True)\n",
    "\n",
    "    # Drop helper/mapping columns\n",
    "    df_save.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Save export files\n",
    "    df_save.to_excel(\"PC2_all_responses_meta_data_one_answer_per_line.xlsx\", index=False)\n",
    "    df_save.to_csv(\"PC2_all_responses_meta_data_one_answer_per_line.csv\", index=False)\n",
    "\n",
    "    print(\"üíæ Saved: PC2_all_responses_meta_data_one_answer_per_line.xlsx / .csv\")\n",
    "    print(\"========== STEP 5 COMPLETE ==========\\n\")\n",
    "\n",
    "    return df_save\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Execute Step 5\n",
    "# ------------------------------------------------------------\n",
    "df_step5_export = Step5_export_transposed_dataset(merged_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41d94c",
   "metadata": {},
   "source": [
    "# Step 6: Group questions by mapping group and divide into separate spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d861172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6 ‚Äî Export Grouped Datasets (3 Output Types)\n",
    "# ============================================================\n",
    "\n",
    "def create_safe_name(text):\n",
    "    \"\"\"\n",
    "    Clean strings so they can safely be used in folder/file names.\n",
    "    Replaces slashes, spaces, and commas with safe characters.\n",
    "    \"\"\"\n",
    "    return str(text).replace(\"/\", \"-\").replace(\" \", \"_\").replace(\",\", \"_\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# A. Export by Taskforce Person ‚Üí Mapping Group ‚Üí Region\n",
    "# ------------------------------------------------------------\n",
    "def export_by_person_and_region(merged_filtered):\n",
    "    \"\"\"\n",
    "    Export A:\n",
    "    For each Taskforce Person ‚Üí each Mapping Group ‚Üí split outputs by Region.\n",
    "    Produces one Excel workbook per (Person √ó Group) with multiple region tabs.\n",
    "    \"\"\"\n",
    "    print(\"üìÅ Export A: Taskforce Person ‚Üí Group ‚Üí Region\")\n",
    "\n",
    "    # List of all taskforce owners assigned to any question\n",
    "    taskforce_people = merged_filtered[\"Taskforce Person\"].dropna().unique()\n",
    "\n",
    "    for person in taskforce_people:\n",
    "\n",
    "        # Create safe folder names\n",
    "        safe_person = create_safe_name(person)\n",
    "        person_folder = f\"output/grouped_responses_by_region/{safe_person}\"\n",
    "        os.makedirs(person_folder, exist_ok=True)\n",
    "\n",
    "        # Subset all responses belonging to this taskforce owner\n",
    "        person_subset = merged_filtered[merged_filtered[\"Taskforce Person\"] == person]\n",
    "\n",
    "        # Loop over each mapping group assigned to that person\n",
    "        for group in person_subset[\"Mapping Group\"].dropna().unique():\n",
    "            safe_group = create_safe_name(group)\n",
    "\n",
    "            # Filter to one specific question group\n",
    "            group_subset = person_subset[person_subset[\"Mapping Group\"] == group].copy()\n",
    "            group_subset.sort_values([\"Question ID\", \"Response ID\"], inplace=True)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Split into quantitative and qualitative responses\n",
    "            # ------------------------------------------------------------\n",
    "            quant_df = group_subset[group_subset[\"Q Type\"] == \"quanti\"].copy()\n",
    "            quali_df = group_subset[group_subset[\"Q Type\"] == \"quali\"].copy()\n",
    "\n",
    "            # Create label maps so columns appear as \"ID - Question text\"\n",
    "            quant_labels = {\n",
    "                row[\"Question\"]: f\"{row['Question ID']} - {row['Question']}\"\n",
    "                for _, row in quant_df.drop_duplicates(subset=[\"Question\"]).iterrows()\n",
    "            }\n",
    "\n",
    "            quali_labels = {\n",
    "                row[\"Question\"]: f\"{row['Question ID']} - {row['Question']}\"\n",
    "                for _, row in quali_df.drop_duplicates(subset=[\"Question\"]).iterrows()\n",
    "            }\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Pivot quantitative and qualitative into wide formats\n",
    "            # ------------------------------------------------------------\n",
    "            quant_pivot = quant_df.pivot(\n",
    "                index=\"Response ID\", columns=\"Question\", values=\"Answer\"\n",
    "            ).reset_index()\n",
    "            quant_pivot.rename(columns=quant_labels, inplace=True)\n",
    "\n",
    "            quali_pivot = quali_df.pivot(\n",
    "                index=\"Response ID\", columns=\"Question\", values=\"Answer\"\n",
    "            ).reset_index()\n",
    "            quali_pivot.rename(columns=quali_labels, inplace=True)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Merge quant + quali so each row contains all answers for that respondent\n",
    "            # ------------------------------------------------------------\n",
    "            if len(quant_df) > 0:\n",
    "                merged = quant_df.merge(quali_pivot, on=\"Response ID\", how=\"left\")\n",
    "            else:\n",
    "                merged = quali_df.merge(quant_pivot, on=\"Response ID\", how=\"left\")\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Export to a multi-sheet Excel workbook split by Region\n",
    "            # ------------------------------------------------------------\n",
    "            output_path = f\"{person_folder}/Question_Group_{safe_group}.xlsx\"\n",
    "            regions = merged[\"Region\"].dropna().unique()\n",
    "\n",
    "            with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "                # Always include the full dataset (\"Overall\")\n",
    "                merged.to_excel(writer, sheet_name=\"Overall\", index=False)\n",
    "\n",
    "                # Create one sheet per region\n",
    "                for region in regions:\n",
    "                    region_subset = merged[merged[\"Region\"] == region]\n",
    "                    sheet_name = create_safe_name(region)[:31]  # Excel limit\n",
    "                    region_subset.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            print(f\"   ‚úì Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f8e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# B. Export by Question Group ‚Üí Region (no taskforce split)\n",
    "# ------------------------------------------------------------\n",
    "def export_by_question_group(merged_filtered):\n",
    "    \"\"\"\n",
    "    Export B:\n",
    "    For each Mapping Group (question group), create a folder and export:\n",
    "      ‚Ä¢ One combined worksheet (\"Overall\")\n",
    "      ‚Ä¢ One sheet per Region\n",
    "    This version does NOT split by taskforce person.\n",
    "    \"\"\"\n",
    "    print(\"üìÅ Export B: Mapping Group ‚Üí Region\")\n",
    "\n",
    "    # Loop over each question group defined in mapping\n",
    "    for group in merged_filtered[\"Mapping Group\"].dropna().unique():\n",
    "\n",
    "        # Safe folder + file names\n",
    "        safe_group = create_safe_name(group)\n",
    "        group_folder = f\"output/split_datasets_by_question_group/Question_Group_{safe_group}\"\n",
    "        os.makedirs(group_folder, exist_ok=True)\n",
    "\n",
    "        # Filter responses belonging to this mapping group\n",
    "        subset = merged_filtered[merged_filtered[\"Mapping Group\"] == group].copy()\n",
    "        subset.sort_values([\"Question ID\", \"Response ID\"], inplace=True)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Split into quantitative and qualitative\n",
    "        # ------------------------------------------------------------\n",
    "        quant_df = subset[subset[\"Q Type\"] == \"quanti\"].copy()\n",
    "        quali_df = subset[subset[\"Q Type\"] == \"quali\"].copy()\n",
    "\n",
    "        # Create column label maps for readability\n",
    "        quant_labels = {\n",
    "            row[\"Question\"]: f\"{row['Question ID']} - {row['Question']}\"\n",
    "            for _, row in quant_df.drop_duplicates(subset=[\"Question\"]).iterrows()\n",
    "        }\n",
    "\n",
    "        quali_labels = {\n",
    "            row[\"Question\"]: f\"{row['Question ID']} - {row['Question']}\"\n",
    "            for _, row in quali_df.drop_duplicates(subset=[\"Question\"]).iterrows()\n",
    "        }\n",
    "\n",
    "        # Pivot quantitative and qualitative\n",
    "        quant_pivot = quant_df.pivot(\n",
    "            index=\"Response ID\", columns=\"Question\", values=\"Answer\"\n",
    "        ).reset_index()\n",
    "        quant_pivot.rename(columns=quant_labels, inplace=True)\n",
    "\n",
    "        quali_pivot = quali_df.pivot(\n",
    "            index=\"Response ID\", columns=\"Question\", values=\"Answer\"\n",
    "        ).reset_index()\n",
    "        quali_pivot.rename(columns=quali_labels, inplace=True)\n",
    "\n",
    "        # Merge quanti + quali (same logic as original code)\n",
    "        if len(quant_df) > 0:\n",
    "            merged = quant_df.merge(quali_pivot, on=\"Response ID\", how=\"left\")\n",
    "        else:\n",
    "            merged = quali_df.merge(quant_pivot, on=\"Response ID\", how=\"left\")\n",
    "\n",
    "        # Clean dataset: replace missing answers with \"N/A\"\n",
    "        merged.replace(np.nan, \"N/A\", inplace=True)\n",
    "\n",
    "        # Output path\n",
    "        output_path = (\n",
    "            f\"{group_folder}/Question_Group_{safe_group}_Responses_By_Region.xlsx\"\n",
    "        )\n",
    "\n",
    "        # Predefined region order\n",
    "        regions = [\n",
    "            \"Europe\", \"Northern America\", \"Asia\",\n",
    "            \"Latin America and the Caribbean\",\n",
    "            \"Africa\", \"Oceania\", \"MENA\"\n",
    "        ]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Write multi-sheet workbook: Overall + each Region\n",
    "        # ------------------------------------------------------------\n",
    "        with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "            merged.to_excel(writer, sheet_name=\"Overall\", index=False)\n",
    "\n",
    "            for region in regions:\n",
    "                region_subset = merged[merged[\"Region\"] == region]\n",
    "                sheet_name = create_safe_name(region)[:31]  # Excel sheet name limit\n",
    "                region_subset.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        print(f\"   ‚úì Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ecc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# C. Export by Taskforce Person ‚Üí Group (no region split)\n",
    "# ------------------------------------------------------------\n",
    "def export_by_person(merged_filtered):\n",
    "    \"\"\"\n",
    "    Export C:\n",
    "    For each Taskforce Person ‚Üí each Mapping Group:\n",
    "      ‚Ä¢ Export one Excel file containing:\n",
    "           - A Qualitative sheet\n",
    "           - A Quantitative sheet\n",
    "    No region-level splitting.\n",
    "    \"\"\"\n",
    "    print(\"üìÅ Export C: Taskforce Person ‚Üí Group (No Region Split)\")\n",
    "\n",
    "    taskforce_people = merged_filtered[\"Taskforce Person\"].dropna().unique()\n",
    "\n",
    "    for person in taskforce_people:\n",
    "        safe_person = create_safe_name(person)\n",
    "        person_folder = f\"output/grouped_responses/{safe_person}\"\n",
    "        os.makedirs(person_folder, exist_ok=True)\n",
    "\n",
    "        # Subset all responses assigned to this person\n",
    "        person_subset = merged_filtered[merged_filtered[\"Taskforce Person\"] == person].copy()\n",
    "\n",
    "        for group in person_subset[\"Mapping Group\"].dropna().unique():\n",
    "            safe_group = create_safe_name(group)\n",
    "\n",
    "            # Filter for one specific mapping group\n",
    "            subset = person_subset[person_subset[\"Mapping Group\"] == group].copy()\n",
    "            subset.sort_values([\"Question ID\", \"Response ID\"], inplace=True)\n",
    "\n",
    "            # Split quanti/quali\n",
    "            quant_df = subset[subset[\"Q Type\"] == \"quanti\"].copy()\n",
    "            quali_df = subset[subset[\"Q Type\"] == \"quali\"].copy()\n",
    "\n",
    "            # Label map for quant\n",
    "            quant_labels = {\n",
    "                row[\"Question\"]: f\"{row['Question ID']} - {row['Question']}\"\n",
    "                for _, row in quant_df.drop_duplicates(subset=[\"Question\"]).iterrows()\n",
    "            }\n",
    "\n",
    "            # Pivot quantitative data\n",
    "            quant_pivot = quant_df.pivot(\n",
    "                index=\"Response ID\", columns=\"Question\", values=\"Answer\"\n",
    "            ).reset_index()\n",
    "            quant_pivot.rename(columns=quant_labels, inplace=True)\n",
    "\n",
    "            # Merge qualitative + quantitative (original logic)\n",
    "            merged = quali_df.merge(quant_pivot, on=\"Response ID\", how=\"left\")\n",
    "\n",
    "            # Output path\n",
    "            output_path = f\"{person_folder}/Question_Group_{safe_group}.xlsx\"\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Write Excel: Qualitative tab + Quantitative tab\n",
    "            # ------------------------------------------------------------\n",
    "            with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "                merged.to_excel(writer, sheet_name=\"Qualitative\", index=False)\n",
    "                quant_df.to_excel(writer, sheet_name=\"Quantitative\", index=False)\n",
    "\n",
    "            print(f\"   ‚úì Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81704a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# MASTER FUNCTION ‚Äî Runs Step 6 (all 3 output types)\n",
    "# ------------------------------------------------------------\n",
    "def Step6_generate_grouped_outputs(merged_filtered):\n",
    "    \"\"\"\n",
    "    Runs all Step 6 exports:\n",
    "      A. Taskforce Person ‚Üí Group ‚Üí Region\n",
    "      B. Question Group ‚Üí Region\n",
    "      C. Taskforce Person ‚Üí Group (no region split)\n",
    "\n",
    "    This does NOT modify logic from the original notebook.\n",
    "    \"\"\"\n",
    "    print(\"========== START STEP 6: Grouped Exports ==========\")\n",
    "\n",
    "    # Check required columns ‚Äî same logic as original code\n",
    "    required_cols = {\n",
    "        \"Mapping Group\", \"Question ID\", \"Response ID\",\n",
    "        \"Taskforce Person\", \"Q Type\", \"Question\", \"Answer\", \"Region\"\n",
    "    }\n",
    "\n",
    "    missing = required_cols - set(merged_filtered.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for Step 6: {missing}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required columns present for Step 6\")\n",
    "\n",
    "    # Run all three export blocks\n",
    "    export_by_person_and_region(merged_filtered)\n",
    "    export_by_question_group(merged_filtered)\n",
    "    export_by_person(merged_filtered)\n",
    "\n",
    "    print(\"========== STEP 6 COMPLETE ==========\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16deeb",
   "metadata": {},
   "source": [
    "## Step 7: Export all quantitative questions onto one separate sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752d7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7 ‚Äî Export ALL Quantitative Questions (One Sheet)\n",
    "# ============================================================\n",
    "\n",
    "def Step7_export_quanti_only(\n",
    "    df_cleaned,\n",
    "    question_map,\n",
    "    meta_cols,\n",
    "    quanti_question_columns,\n",
    "    columns_to_drop,\n",
    "    output_dir=\"output\"\n",
    "):\n",
    "    print(\"========== START STEP 7: Export ALL Quantitative Questions ==========\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Filter mapping down to only quantitative questions\n",
    "    # --------------------------------------------------------\n",
    "    quanti_mapping = question_map[\n",
    "        question_map[\"Question\"].isin(quanti_question_columns)\n",
    "    ].copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Melt to long format (one row per answer)\n",
    "    # --------------------------------------------------------\n",
    "    df_long = (\n",
    "        df_cleaned[meta_cols + quanti_question_columns]\n",
    "        .reset_index()\n",
    "        .melt(id_vars=meta_cols, var_name=\"Question\", value_name=\"Answer\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Merge with mapping doc\n",
    "    # --------------------------------------------------------\n",
    "    quanti_filtered = pd.merge(df_long, quanti_mapping, on=\"Question\", how=\"left\")\n",
    "\n",
    "    # Remove non-answers and useless ‚Äúindex‚Äù dummy column\n",
    "    quanti_filtered = quanti_filtered[\n",
    "        (quanti_filtered[\"Question\"] != \"index\") &\n",
    "        (quanti_filtered[\"Answer\"].notna())\n",
    "    ]\n",
    "\n",
    "    print(f\"üìÑ Final quantitative long-format shape: {quanti_filtered.shape}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4. Save FULL version including mapping columns\n",
    "    # --------------------------------------------------------\n",
    "    quanti_filtered.to_excel(\n",
    "        f\"{output_dir}/PC2_all_quantitative_responses_one_answer_per_line_for_analysis.xlsx\",\n",
    "        index=False\n",
    "    )\n",
    "    quanti_filtered.to_csv(\n",
    "        f\"{output_dir}/PC2_all_quantitative_responses_one_answer_per_line_for_analysis.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"üíæ Saved quantitative dataset WITH mapping (analysis version)\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5. Drop mapping columns (in-place)\n",
    "    # --------------------------------------------------------\n",
    "    quanti_filtered.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6. Save clean version (only meta + Question + Answer)\n",
    "    # --------------------------------------------------------\n",
    "    quanti_filtered.to_excel(\n",
    "        f\"{output_dir}/PC2_all_quantitative_responses_one_answer_per_line.xlsx\",\n",
    "        index=False\n",
    "    )\n",
    "    quanti_filtered.to_csv(\n",
    "        f\"{output_dir}/PC2_all_quantitative_responses_one_answer_per_line.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"üíæ Saved quantitative dataset CLEANED (no mapping columns)\")\n",
    "    print(\"========== STEP 7 COMPLETE ==========\\n\")\n",
    "\n",
    "    return quanti_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e8b128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START STEP 7: Export ALL Quantitative Questions ==========\n",
      "üìÑ Final quantitative long-format shape: (1108, 23)\n",
      "üíæ Saved quantitative dataset WITH mapping (analysis version)\n",
      "üíæ Saved quantitative dataset CLEANED (no mapping columns)\n",
      "========== STEP 7 COMPLETE ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Execute Step 7\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "quanti_output = Step7_export_quanti_only(\n",
    "    df_cleaned=df_cleaned,\n",
    "    question_map=question_map,\n",
    "    meta_cols=meta_cols,\n",
    "    quanti_question_columns=quanti_questions,   # from Step 2\n",
    "    columns_to_drop=columns_to_drop,            # from Step 5\n",
    "    output_dir=\"output\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
